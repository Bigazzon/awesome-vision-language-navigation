# awesome-embodied-ai

Vision-and-Language Navigation (VLN) has becoming an important topic. Create an issue or email to jgu110@ucsc.edu if you have any suggestions on this repo!
Please also check our VLN survey paper: Vision-and-Language Navigation:A Survey of Tasks, Methods, and Future Directions.

## Datasets

- Visual Navigation for Mobile Robots: A Survey [paper](https://link.springer.com/article/10.1007/s10846-008-9235-4)

- Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments [paper](https://arxiv.org/abs/1711.07280) 

- Stay on the Path: Instruction Fidelity in Vision-and-Language Navigationn [paper](https://doi.org/10.18653/v1/P19-1181)

- Cross-Lingual Vision-Language Navigation [paper](http://arxiv.org/abs/1910.11301)

- Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding [paper](https://arxiv.org/abs/2010.07954)

- Landmark-RxR: Solving Vision-and-Language Navigation with Fine-Grained Alignment Supervision [paper](https://proceedings.neurips.cc/paper/2021/hash/0602940f23884f782058efac46f64b0f-Abstract.html)

- Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments [paper](https://arxiv.org/abs/2004.02857) 

- Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation [paper](https://arxiv.org/abs/2104.10674)

- Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments [paper](https://doi.org/10.1109/CVPR.2019.01282)

- The Streetlearn Environment and Dataset [paper](https://arxiv.org/abs/1903.01292)

- Retouchdown: Releasing Touchdown on StreetLearn as a Public Resource for Language Grounding Tasks in Street View [paper](https://doi.org/10.18653/v1/2020.splu-1.7)

- Learning To Follow Directions in Street View [paper](https://arxiv.org/abs/1903.00401)

- Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory [paper](https://arxiv.org/abs/1910.02029)

- Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction [paper](https://arxiv.org/abs/1809.00786)

- Following High-level Navigation Instructions on a Simulated Quadcopter with Imitation Learning [paper](https://arxiv.org/abs/1806.00047)
 
- Building Generalizable Agents with a Realistic and Rich 3D Environment [paper](https://openreview.net/forum?id=rkaT3zWCZ)

- REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments [paper](https://arxiv.org/abs/1904.10151)

- SOON: Scenario Oriented Object Navigation with Graph-based Exploration [paper](https://arxiv.org/abs/2103.17138)

- AI2-THOR: An Interactive 3D Environment for Visual AI [paper](https://arxiv.org/abs/1712.05474)

- ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks [paper](https://arxiv.org/abs/1912.01734)

- Just Ask:An Interactive Learning Framework for Vision and Language Navigation [paper](https://arxiv.org/abs/1912.00915)

- Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention [paper](https://arxiv.org/abs/1812.04155)




## Methods

## Instruction Following (e.g. VLN):

- R2R (Room To Room): Matterport3D: Learning from {RGB-D} Data in Indoor Environments [paper](https://arxiv.org/abs/1711.07280) [project](https://bringmeaspoon.org/)

- Room-Across-Room (RxR): Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding [paper](https://arxiv.org/abs/2010.07954) [project](https://ai.google.com/research/rxr/)

- Habitat: Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments [paper](https://arxiv.org/abs/2004.02857) [project](https://jacobkrantz.github.io/vlnce/)

- Habitat Workshop: 2021 Habitat Challenge launches to advance embodied AI research [project](https://aihabitat.org/challenge/2021)

- TouchDown: Touchdown: Natural Language Navigation and Spatial Reasoning in Visual Street Environments
 [paper](https://arxiv.org/abs/1811.12354) [project](https://github.com/lil-lab/touchdown)

- ALFRED: ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks [paper](https://arxiv.org/abs/1912.01734) [project](https://askforalfred.com/)

- REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments [paper](https://arxiv.org/abs/1904.10151) [project](https://github.com/YuankaiQi/REVERIE)

- Drone navigation: Works from [Valts Blukis](http://www.cs.cornell.edu/~valts/)

- Refer360: A Referring Expression Recognition Dataset in 360Â° Images [paper](https://www.aclweb.org/anthology/2020.acl-main.644.pdf) [project](https://github.com/volkancirik/refer360/)

- VNLA: Vision-Based Navigation With Language-Based Assistance via Imitation Learning With Indirect Intervention [paper](https://arxiv.org/abs/1812.04155) [project](https://github.com/debadeepta/vnla)

## Dialog-related

- CVDN: Vision-and-Dialog Navigation [paper](https://arxiv.org/abs/1907.04957) [project](https://cvdn.dev/)

- Localization from embedded dialog: Where Are You? Localization from Embodied Dialog [paper](https://arxiv.org/abs/2011.08277) [project](https://github.com/batra-mlp-lab/WAY)

- RobotSlang: The RobotSlang Benchmark: Dialog-guided Robot Localization and Navigation [paper](https://arxiv.org/abs/2010.12639) [project](https://umrobotslang.github.io/)

## Audio-related

- SoundSpaces: SoundSpaces: Audio-Visual Navigation in 3D Environments [paper](https://arxiv.org/abs/1912.11474) [project](http://vision.cs.utexas.edu/projects/audio_visual_navigation/)

- ThreeDWorld: ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation [paper](https://arxiv.org/abs/2007.04954) [project](http://www.threedworld.org/)

## Manipulation

- PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable Physics [paper](https://openreview.net/pdf?id=xCcdBRQEDW) [project](http://plasticinelab.csail.mit.edu/) 

- ThreeDWorld: ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation [paper](https://arxiv.org/abs/2007.04954) [project](http://www.threedworld.org/)

- SAPIEN: A SimulAted Part-based Interactive ENvironment [project](https://sapien.ucsd.edu/)

- RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation [paper](https://arxiv.org/abs/1811.02790) [project](https://roboturk.stanford.edu/)

- RLBench: The Robot Learning Benchmark & Learning Environment [paper](https://arxiv.org/abs/1909.12271) [project](https://sites.google.com/view/rlbench)

- CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning. [project](https://sites.google.com/view/causal-world/home)

- Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning. [project](https://meta-world.github.io/)

## Other embodied tasks and datasets

- Gibson: Gibson Env: Real-World Perception for Embodied Agents [paper](https://arxiv.org/abs/1808.10654) [project](http://gibsonenv.stanford.edu/)

- Replica: The Replica Dataset: A Digital Replica of Indoor Spaces [paper](https://arxiv.org/abs/1906.05797) [project](https://github.com/facebookresearch/Replica-Dataset)

- Rearrangement: A Challenge for Embodied AI [paper](https://arxiv.org/abs/2011.01975)

- MultiON: Benchmarking Semantic Map Memory using Multi-Object Navigation [paper](https://arxiv.org/abs/2012.03912) [project](https://shivanshpatel35.github.io/multi-ON/)






